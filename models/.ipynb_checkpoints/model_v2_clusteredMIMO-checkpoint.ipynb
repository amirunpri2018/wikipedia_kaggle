{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.cluster as skc\n",
    "import sklearn.preprocessing as skp\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_keys = pd.read_csv('../Processing/FrameContainer/DataRows/Data_keys.csv', header=None)\n",
    "data_keys = data_keys.iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = np.loadtxt('../Processing/DTW_Matrix.out.gz', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Hierarchical clustering on the matrix \n",
    "cluster_ward = skc.AgglomerativeClustering(n_clusters=10, affinity='euclidean', connectivity=K, compute_full_tree=True, linkage='ward')\n",
    "# cluster_complete = skc.AgglomerativeClustering(n_clusters=10, affinity='euclidean', connectivity=K, compute_full_tree=True, linkage='complete')\n",
    "# cluster_average = skc.AgglomerativeClustering(n_clusters=10, affinity='euclidean', connectivity=K, compute_full_tree=True, linkage='average')\n",
    "\n",
    "# Predict the class labels\n",
    "labels_ward = cluster_ward.fit_predict(K)\n",
    "# labels_complete = cluster_complete.fit_predict(K)\n",
    "# labels_average = cluster_average.fit_predict(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in Offline Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpful dict structure\n",
    "labeled_data = dict(list(zip(data_keys, labels_ward)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting join process for Query Chunk: 0\n",
      "Starting join process for Query Chunk: 1\n",
      "Starting join process for Query Chunk: 2\n",
      "Starting join process for Query Chunk: 3\n",
      "Starting join process for Query Chunk: 4\n",
      "Starting join process for Query Chunk: 5\n",
      "Starting join process for Query Chunk: 6\n",
      "Starting join process for Query Chunk: 7\n",
      "Starting join process for Query Chunk: 8\n",
      "Starting join process for Query Chunk: 9\n",
      "Starting join process for Query Chunk: 10\n",
      "Starting join process for Query Chunk: 11\n",
      "Starting join process for Query Chunk: 12\n",
      "Starting join process for Query Chunk: 13\n",
      "Starting join process for Query Chunk: 14\n"
     ]
    }
   ],
   "source": [
    "dataResult_PATH = '../Processing/FrameContainer/DataRows/Data_Keys.csv'\n",
    "data_keys = pd.read_csv(dataResult_PATH, delimiter='\\n', header=None)\n",
    "data_keys = data_keys[0].values.tolist()\n",
    "\n",
    "for idx in range(0,15):\n",
    "    print(\"Starting join process for Query Chunk: %d\" % idx)\n",
    "    \n",
    "    outResult_PATH = '../Processing/FrameContainer/OutRows/'\n",
    "    queryKey_PATH = '../Processing/FrameContainer/QueryRows/'\n",
    "    \n",
    "    out_FNAME = 'Out_Rows_'\n",
    "    query_FNAME = 'Query_Keys_'\n",
    "    \n",
    "    queryIndices = pd.read_csv(queryKey_PATH + query_FNAME + str(idx) + '.csv', delimiter='\\n', header=None)\n",
    "    searchResults = pd.read_csv(outResult_PATH + out_FNAME + str(idx) + '.txt', delimiter='\\n', header=None)\n",
    "    \n",
    "    queryIndices = queryIndices[0].values.tolist()\n",
    "    searchResults = searchResults[0].values.tolist()\n",
    "    # Map search results to corresponding data row index\n",
    "    mappedResults = [data_keys[i] for i in searchResults]\n",
    "    \n",
    "    for idx, q in enumerate(queryIndices):\n",
    "        labeled_data[q] = labeled_data[mappedResults[idx]]\n",
    "\n",
    "class_data = defaultdict(list)\n",
    "for k, v in labeled_data.items():\n",
    "    class_data[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 | Class size: 982\n",
      "Label 1 | Class size: 5908\n",
      "Label 2 | Class size: 2915\n",
      "Label 3 | Class size: 2991\n",
      "Label 4 | Class size: 21433\n",
      "Label 5 | Class size: 20520\n",
      "Label 6 | Class size: 89502\n",
      "Label 7 | Class size: 247\n",
      "Label 8 | Class size: 186\n",
      "Label 9 | Class size: 379\n"
     ]
    }
   ],
   "source": [
    "for k in class_data.keys():\n",
    "    print(\"Label %d | Class size: %d\" % (k, len(class_data[k])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matthew/miniconda3/envs/bleedingEdge/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "'''\n",
    "alg: XGBClassifier() or XGBRegressor()\n",
    "dtrain: the entire training dataframe\n",
    "predictors: list of feature columns in dtrain\n",
    "target: column we want to classify\n",
    "'''\n",
    "\n",
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=3, early_stopping_rounds=4):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=target)\n",
    "        cvresult = xgb.cv(xgb_param, \n",
    "                          xgtrain, \n",
    "                          num_boost_round=alg.get_params()['n_estimators'],\n",
    "                          nfold=cv_folds,\n",
    "                          metrics='logloss', \n",
    "                          early_stopping_rounds=early_stopping_rounds, \n",
    "                          verbose_eval=True)\n",
    "        \n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], target, eval_metric='logloss')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(target, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=50,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Indirect Models on Single Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData_df = pd.read_csv('../input/train_1.csv')\n",
    "rawData_df.fillna(value=0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's just attempt for Class 9 \n",
    "CLASS_SIZE = len(class_data[9])\n",
    "FIXED_WIDTH = 25\n",
    "FIXED_ROW_CAP = 10e3\n",
    "SAMPLE_PER_ROW = floor(FIXED_ROW_CAP / CLASS_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_slice_df = rawData_df.ix[class_data[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NR, NC = class_slice_df.shape\n",
    "\n",
    "jointFrame = pd.DataFrame()\n",
    "\n",
    "# TRAIN A MODEL FOR EACH GAP\n",
    "for gap in range(0,60):\n",
    "    \n",
    "    # Create the training samples by going through each row\n",
    "    for spridx in range(0, SAMPLE_PER_ROW):\n",
    "        indices = list(range(NC-2-gap-spridx-FIXED_WIDTH, NC-1-gap-spridx))\n",
    "#         print(NC-2-gap-spridx-FIXED_WIDTH)\n",
    "#         print(NC-2-gap-spridx)\n",
    "#         print(NC-1-spridx)\n",
    "        tmp_slice = class_slice_df.iloc[:,indices]\n",
    "        tmp_slice.columns = list(range(0,FIXED_WIDTH+1))\n",
    "            # Alway update the joint frame\n",
    "        if (jointFrame.empty):\n",
    "            jointFrame = tmp_slice\n",
    "        else:\n",
    "            jointFrame = jointFrame.append(tmp_slice, ignore_index=True)\n",
    "\n",
    "    jointFrame.rename(columns={FIXED_WIDTH:'y'}, inplace=True)\n",
    "    break\n",
    "    \n",
    "    # XGBoost SVM on the generated training samples\n",
    "    \n",
    "    #Choose all predictors except target & IDcols\n",
    "\n",
    "\n",
    "modelfit(xgb1, df_X, SEMANTIC_COL_LABELS + TREEKERNEL_COL_LABELS, df_y.values)\n",
    "    \n",
    "    # Map the new datapoints to page-names\n",
    "    \n",
    "    # Concatenate the training samples on an empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
