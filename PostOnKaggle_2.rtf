{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Apologies for the late response. Let me give an answer in depth.\
\
As to your first question, I did a literature review and DTW authors (Keogh et. al) report superior performance to feature based methods on a number of datasets. This isn\'92t based on any personal experience in competitions - I just took stock of the current literature. \
\
You are correct with the 1000 rows! The only reason is computational. There are ~140000 rows and it takes ~ 10e-3 seconds to compute a pair. That\'92s 10e5 * 10e5 * 10e-3 = 10e7. Unreasonable amount of time to do a complete pairwise matrix. So the idea is do a pairwise matrix on a sample and use nearest neighbor to assign the rest. Obviously it isn\'92t perfect - but I\'92m unaware of a more principled method for clustering based on a sample. \
\
So from my reading DTW 
\b necessitates
\b0  either z-scaling or MinMax scaling. The whole idea is that the it isn\'92t about magnitude of the time series but the distortion - i.e. where the spikes in data overlap. You\'92re asking great questions - I just don\'92t have answer backed by evidence or experience. }