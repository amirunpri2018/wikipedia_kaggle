{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLoad the training dataframe\\nSample for 10000 rows\\nConvert the rows to appropriate format for fast DTW \\nPairwise DTW on all samples - 10^6 copmutations, ~10^-3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load the training dataframe\n",
    "Sample for 10000 rows\n",
    "Convert the rows to appropriate format for fast DTW \n",
    "Pairwise DTW on all samples - 10^6 copmutations, ~10^-3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics.pairwise as skm\n",
    "import sklearn.preprocessing as skp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastdtw\n",
    "from timeit import default_timer as timer\n",
    "import sklearn.cluster as skc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dtw(arr1, arr2):\n",
    "    error, _ = fastdtw.fastdtw(arr1, arr2, radius=1, dist=2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in and Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145063, 551)\n"
     ]
    }
   ],
   "source": [
    "rawData_df = pd.read_csv('input/train_1.csv')\n",
    "print(rawData_df.shape)\n",
    "# Fill all NaN with zeros\n",
    "rawData_df.fillna(value=0.0,inplace=True)\n",
    "rawData_df.drop('Page',axis=1,inplace=True)\n",
    "\n",
    "# Shuffle the dataframe \n",
    "rawData_df = rawData_df.sample(frac=1)\n",
    "\n",
    "# Could also scale [0,1/N] or z-normalize\n",
    "scaled_data = skp.MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(rawData_df)\n",
    "\n",
    "scaledData_df = pd.DataFrame(data=scaled_data,            # values\n",
    "                index=rawData_df.index.values,            # 1st column as index\n",
    "                columns=rawData_df.columns)               # 1st row as the column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataRows_df = scaledData_df.iloc[:1000]\n",
    "\n",
    "# Write some sample rows to file\n",
    "dataRows_df.to_csv(path_or_buf='Processing/Data_Rows.txt', header=False, sep=' ',index=False, index_label=False, line_terminator=' ', na_rep=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitPoints = list(range(10000, len(scaledData_df), 10000))\n",
    "for i, val in enumerate(splitPoints):\n",
    "    tmpQueryRows = scaledData_df.iloc[val:val+10000]\n",
    "    filePath = 'Processing/Query_Rows_' + i + '.txt' \n",
    "    queryRows_df.to_csv(path_or_buf=filePath, header=False, sep=' ',index=False, index_label=False, line_terminator=' ', na_rep=0.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write our files for offline processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pairwise DTF of all \"data-rows\" (10000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383.9774313321104\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'savetext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b6e54f810156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing/DTW_Matrix.out.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# FASTDTW ON SCALE 4E-4 PER PAIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'savetext'"
     ]
    }
   ],
   "source": [
    "# n_jobs = -1 to turn on parallel\n",
    "start = timer()\n",
    "K = skm.pairwise_kernels(dataRows_df, metric=dtw, n_jobs=1)\n",
    "end = timer()\n",
    "print(end - start)      \n",
    "\n",
    "np.savetxt(\"Processing/DTW_Matrix.out.gz\", K, delimiter=',')\n",
    "\n",
    "# FASTDTW ON SCALE 4E-4 PER PAIR\n",
    "# AT 10^3 SAMPLES, 10^6 ELS, SO 10^2 SECONDS. Tolerance for production up to 10^4 elements \n",
    "# several hours of runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"Processing/DTW_Matrix.out.gz\", K, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run Hierarchical clustering on the matrix \n",
    "cluster = skc.AgglomerativeClustering(n_clusters=10, affinity='euclidean', connectivity=K, compute_full_tree=True, linkage='ward')\n",
    "labels = cluster.fit_predict(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
